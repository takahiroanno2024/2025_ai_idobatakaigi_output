# 東京都として取るべきAIガバナンス体制・ルールの整備方針

<div style="margin-bottom: 20px;">
<a href="https://large-scale-conversation-sandbox.discourse.group/t/topic/367" style="display: inline-block; padding: 10px 20px; margin-right: 10px; background-color: #0366d6; color: white; text-decoration: none; border-radius: 5px; font-weight: bold;">Discourse で議論する</a>
<a href="https://github.com/takahiroanno2024/2025_ai_idobatakaigi_output/blob/main/docs/4_%E3%82%AC%E3%83%90%E3%83%8A%E3%83%B3%E3%82%B9%E3%83%AB%E3%83%BC%E3%83%AB%E3%81%AE%E6%95%B4%E5%82%99.md" style="display: inline-block; padding: 10px 20px; background-color: #24292e; color: white; text-decoration: none; border-radius: 5px; font-weight: bold;">GitHub で編集する</a>
</div>

# 背景
東京都は、AI技術を利用した行政サービスの効率化を進める必要がある一方で、倫理的・法的なリスクを管理するためのガバナンス体制の整備が求められています。AI活用を進める中、プライバシー保護や不適切な利用を避けるためのルール作り、AIの判断が誤った場合の責任の所在などの不明瞭さなどの課題があり、適切なガバナンスを確立することが急務となっています。本提言の作成にあたり、AIチャット、掲示板、X、YouTube等を通じて有識者の意見を収集し、教育格差の是正の必要性や、AI活用の有効性、導入にあたってのハードル、プライバシーと監視のバランス、AI導入の優先順位などの論点を検討しました。既に海外ではシアトル市やカリフォルニア州、アムステルダム市などで、AIのガバナンスに関するルールが整備されており、特に高リスク業務においては人間による最終確認を必須とする方針が取られています。これらを踏まえ、東京都でもAI導入に際して透明性・責任の明確化を重視し、ガバナンス体制を強化する必要があると考えられます。

# 提案要旨
東京都がAIを行政業務に導入するにあたっては、適切なガバナンス・ルールの整備が不可欠です。まず、業務をリスクに応じて分類し、高リスク業務には人間による最終確認を義務付けることが重要です。次に、AI活用の責任は最終的に人間が担うことを明文化し、都民への信頼性を確保します。また、外部の専門家を含む中立的な監視・評価体制を整え、AI活用における透明性と説明責任を強化します。さらに、AIルールは機械可読かつ柔軟に更新できる形式で整備し、技術進化や社会変化に対応できる体制を構築します。最後に、職員・市民向けのAIリテラシーを向上させることで、全体的なAIの適切な利用を促進します。

# 方針案

## 方針1：リスクに応じた業務分類と利用制限の明確化

### 方針の詳細
都民に対する影響の大きさや、利用されるデータの性質、判断の客観性に応じて、行政業務を分類し、生成AIの利用可否やガバナンスの強度を定める。具体的には、以下のような三軸で業務を整理する：
- 【影響度】都民の生命・健康・財産に重大な影響を与える業務かどうか
- 【客観性】業務における判断基準が数値化・定量化可能かどうか
- 【データ特性】個人情報や機密性の高いデータを扱うかどうか

これらの評価に基づき、「高リスク業務（人間による最終判断必須）」「中リスク業務（限定的にAI利用可）」「低リスク業務（AI主体で運用可能）」などの分類を導入する。

### 方針とすべき根拠（得られうるベネフィット等）
- **安全性の担保**：都民の権利や安全に関わる業務に対して、適切なガバナンスが可能になる。
- **透明性と説明責任の強化**：なぜAIを導入するのか、なぜ導入しないのかを明確に説明できる。
- **リソース最適化**：AIが適用できる範囲とできない範囲を明確化することで、効率的な行政運営が可能になる。
- **AI利活用の推進と慎重な導入の両立**：イノベーション促進とリスク回避を両立できる。

### 考えうる課題やリスク
- 評価基準があいまいだと、恣意的な分類・運用になる可能性がある。
- 主観評価の業務（福祉、教育など）の扱いが難しくなる。
- 分類が複雑になると、導入・運用コストがかさむ。
- 判断が固定化しすぎると、新しい技術の導入機会を逃す可能性がある。

### 挙げられた意見（掲示板より抜粋）
- 「都民の生命、健康、財産などに重大な影響を及ぼす事業については出力データのチェックや最終的な意思決定は人間が行うなどの規制が考えられる」 – *Hirayama*
- 「影響度を基準とするのが妥当だと考えるが、その評価方法には慎重な検討が必要であると捉えている。例えば、医療や公共インフラのように明確なリスクがある分野は判断しやすいが、教育や福祉のように影響が長期的に現れる分野では、どのようにリスクを測定すべきか難しい。また、影響度が高いからといってAIの導入を制限しすぎれば、技術革新の機会を逃す可能性がある」 – *NAGAI*
- 「業務の仕分けの観点で、主観によってそれぞれの価値判断が変わる事業とそうでない事業に分けられそう」 – *NakataniHibiki*
- 「AI利用におけるリスク度に応じた業務分類は、AIガバナンスの根幹をなす重要な論点」 – *jujunjun110*

### 海外事例
- **シアトル市**：職員がAIを使う際の利用ガイドラインにおいて、「利用する業務の性質に応じて人間の最終確認を必須とする」と明記。高リスク分野ではAIの出力内容に必ず人間の目視チェックを求めている。
- **カリフォルニア州**：医療分野では「AIが作成した診療メッセージには出所を明示し、必ず人間の医療従事者が最終的な対応を行うこと」を義務付ける州法AB3030を制定。業務の性質によって運用強度を明確にしている。
- **アムステルダム市**：職員による生成AIの使用を原則禁止としたが、一方で「Chat Amsterdam」というパイロットを通じて、業務内容に応じた慎重な導入の検証を進めている。


## 方針2：最終責任は人間にあることの明文化

### 方針の詳細
行政におけるAI活用に際し、AIによって生成・出力された情報や判断については、**必ず人間が最終的な責任を持つこと**を原則として明記する。とくに都民の権利・義務に関わる判断、医療・福祉・教育などの分野では、人間によるチェック・承認・意思決定を義務づける。また、リスクが低い領域ではAIの自動出力も許容されるが、その場合も人間の責任の範囲と明示義務を定める。

### 方針とすべき根拠（得られうるベネフィット等）
- **説明責任の明確化**：AIによる判断で不利益を被った場合、都として責任主体を明示できる。
- **都民の信頼確保**：人間が最終的に責任を持つことで、AI活用に対する不安を和らげる。
- **制度的リスクの低減**：法的・倫理的責任の所在が不明確なまま運用されるリスクを避けられる。
- **リスクある業務への歯止め**：医療・公共安全等における自動判断の乱用を防止。

### 考えうる課題やリスク
- **人間による形骸的なチェック**が常態化すると、かえって責任回避の温床となりうる。
- **人手による確認工数**が大幅に増え、効率化の利点が損なわれる可能性がある。
- **AIの方が正確な場合**でも、常に人間が上位にあることで技術の活用可能性が制限される。
- 将来的に、特定の分野でAIの判断精度等が人間を上回る可能性があり、その際に「常に人間が最終責任を負う」という原則が、最適な判断や効率化の妨げとなる可能性がある。

### 挙げられた意見（掲示板より抜粋）
- 「都民の生命、健康、財産などに重大な影響を及ぼす事業においては、人間の目視チェックと承認を経て表示させるようにすること」 – *jun_as_member*
- 「補助金の審査など、重要な判断についてはAIだけでなく人間が最終確認することを義務化するべき」 – *NAGAI*
- 「判断が人間よりもAIの方が優れている場合もある。『人間が常に最終判断』という前提に疑問を持つ」 – *nishio*
- 「AIと人間の判断精度を比較し、エビデンスベースでどちらが合理的かを評価すべき」 – *nishio*
- 「『正しい確率の高いAIの判断を、正しい確率の低い人間でチェックする』という無駄な作業に税金が使われるのは有害だ」

### 海外事例
- **シアトル市**：生成AIの利用に関して「職員がAI出力の内容について最終責任を負う」と明記し、AIで作成された文書・回答は必ず人間が確認・承認することを義務づけている。
- **ボストン市**：職員に対し「AIが生成した内容を鵜呑みにせず、必ず検証すること」を推奨。判断内容に差別的表現や誤情報が含まれていないか、確認を徹底。
- **カリフォルニア州・医療分野（AB 3030）**：診療に関連するAI出力は必ず「AIによる生成である」旨を表示し、人間の医療担当者へのアクセス保証を義務づけ。AIは医師の代替ではなく補助であることを法律で明確にしている。
- **サンフランシスコ市**：職員によるAI出力の使用について、「常にファクトチェックを行い、内容に責任を持つこと」を義務づけており、警察・行政含む全部署に適用。




## 方針3：中立性と透明性を担保する監視・評価体制の構築

### 方針の詳細
AIの活用において、行政内部だけで完結しない**中立的かつ多様な視点による監視・評価体制**を構築する。大学・企業・市民団体・弁護士・技術専門家などから構成される**外部倫理監視委員会**の設置を検討し、AI活用における方針・運用の妥当性をチェックする仕組みとする。また、プロンプトや出力の透明化、使用実績の記録・公開などにより、市民からの監視や意見提出を可能にする仕組みも併設する。

### 方針とすべき根拠（得られうるベネフィット等）
- **客観性の確保**：行政内部では拾いきれない倫理的・社会的観点からのレビューが可能になる。
- **透明性向上による信頼形成**：都民がガバナンスの仕組みにアクセスできることで、安心してAI活用を受け入れられる。
- **リスクの早期発見**：外部の目が入ることで、予期しないトラブルや判断ミスの芽を早期に摘むことができる。
- **政策の柔軟な改善が可能に**：フィードバックを得やすく、アジャイルな見直しにもつながる。

### 考えうる課題やリスク
- **監視機関の中立性の担保**：利害関係者が入りすぎると、判断の公正性が疑われるおそれ。
- **過度な監査負荷による導入停滞**：レビューや開示に時間がかかりすぎ、行政の迅速性が損なわれるリスク。
- **情報公開とセキュリティの両立**：プロンプトやAI利用実態をどこまで開示すべきかの線引きが難しい。

### 挙げられた意見（掲示板より抜粋）
- 「チェック機能を果たす中立的な組織について、大学・企業・市民団体・技術の専門家を含めた多様なメンバーで構成された倫理監視委員会を設置できると良い」 – *NAGAI*
- 「中立的な組織を組成することについては、弁護士や研究者、民間企業のAI利用者等様々な属性の人から成る方が良いと思う」 – *Hirayama*
- 「プロンプトを公開して市民にチェックしてもらうことも一手」 – *Takatsugu_Imaizumi*
- 「プロンプトは情報公開条例にいう行政文書にあたると考えられる」 – *Takatsugu_Imaizumi*

### 海外事例
- **サンフランシスコ市**：市が使用するすべてのAIツールについて、目的・精度・バイアス・限界などを記載した「AIインベントリ（公開一覧）」の作成を条例で義務化。市民が行政によるAIの活用状況を把握できる。
- **シアトル市**：AI活用に際し、外部サービスの利用についても契約によってガイドライン順守を義務づけ。職員が出力結果に責任を持つとともに、内容と出所の公開を徹底。
- **深圳市（中国）**：都市独自のAI倫理委員会を設置し、公共部門におけるAIの活用実態をモニタリング。国家規制に基づきながらも地方での実践的な監視体制を構築。
- **G20スマートシティ連合の報告**：多くの都市がガバナンス整備の初期段階にあり、「市民の信頼を得るには透明性の確保と市民参加が鍵」とされている。


## 方針4：機械可読かつ柔軟に更新できるルール整備

### 方針の詳細
生成AIの活用ルールは、単に文書で策定するだけでなく、**AIシステムが理解・参照できる形式（＝機械可読）での整備**を進める。また、技術や社会情勢の変化に迅速に対応するため、**定期的な見直しが可能な「アジャイル型ガバナンス」**を採用する。ルール本体はGitHub等でホスティングし、修正履歴を管理しながら、市民・職員・専門家が改善に参加できる設計とする。

### 方針とすべき根拠（得られうるベネフィット等）
- **AIの自律的なルール遵守**：プロンプトにルールを埋め込むことで、生成物が自動的にガイドラインを反映するようになる。
- **アップデートの迅速化**：法整備や社会情勢の変化にあわせてタイムリーな見直しが可能に。
- **市民参加と透明性の強化**：更新の過程を公開し、フィードバックを受けることで、より実効性のあるルールへと育てられる。
- **マルチバージョン対応**：要約版・グラフィカル版・多言語翻訳版など派生版を自動生成できる設計も可能。

### 考えうる課題やリスク
- **技術的整備コストの増大**：ルールを機械可読にするための追加開発や保守が必要。
- **市民にとっての可読性との両立**：機械可読性を高めると、逆に人間にとって読みにくくなる懸念がある。
- **誤作動リスク**：ルールの記述ミスがAIの出力に予期せぬ影響を与える可能性がある。

### 挙げられた意見（掲示板より抜粋）
- 「AIを含む機械によって読解可能なルールにすることはできるか。プロンプトに含めることで生成物がすべてガバナンス・ルールを順守させることができるのではないか」 – *jujunjun110*
- 「llms.txt の中には具体例を何個かいれることによって、いろんな基盤モデルで参照されたときに自然と few-shot で回答できるようにすると精度が上がりそう」 – *takahiroanno*
- 「updateプロセスをきちんと定めておく & クイックにアップデートできるようにすると良さそう」 – *takahiroanno*
- 「GitHubにホスティングして、PRを集め、マージ権限を持つ所管部門を定めて定期的に意思決定するオペレーションが一案」 – *takahiroanno*

### 海外事例
- **カリフォルニア州**：生成AIの活用方針を州全体で統一し、「GenAIガイドライン」などの技術文書をオンラインで公開。各機関での継続的アップデートとリスク評価プロセスを推奨。
- **サンフランシスコ市**：2024年にAI使用状況を公開する条例を制定し、インベントリ情報を常に最新の状態で保つための更新体制を整備。
- **EU（AI法）**：2025年施行予定のAI法では、生成AIに対して「継続的な監視・修正義務」を課しており、動的ガバナンスの導入が前提とされている。
- **G20スマートシティ連合**：多くの都市で「固定的なガイドラインではなく、社会的対話を踏まえた柔軟な更新が不可欠」とする方針を共有。


## 方針5：職員・市民向けAIリテラシーと説明責任の徹底

### 方針の詳細
生成AIを東京都の行政業務で活用するにあたり、職員および市民双方に対する**AIリテラシーの向上**と、**AIが関与した情報・判断であることの明示（説明責任）**を徹底する。具体的には以下のような施策を講じる：
- 都庁職員に対するAI活用とガバナンスに関する研修の実施
- AIによる出力であることを示すディスクレーマー（例：「この文章はAIによって生成されました」）の明示
- 市民からの問い合わせに対し、AIの利用範囲や根拠となるルールの説明が可能な体制づくり

### 方針とすべき根拠（得られうるベネフィット等）
- **不信感や誤解の解消**：AIの関与を明示することで、市民が内容を正しく受け止めやすくなる。
- **ガバナンスの実効性確保**：職員のリテラシーが向上すれば、ルールが形骸化せず実際に運用される。
- **都民の教育的機会の創出**：説明責任を果たす中で、市民にもAIの使い方やリスクが伝わる。
- **政策への市民参加の促進**：市民がAIの仕組みや役割を理解すれば、より建設的な意見や参加が可能になる。

### 考えうる課題やリスク
- **研修リソースの確保**：職員の全員に研修を行うには時間・コストがかかる。
- **AI出力表示の過剰表示問題**：低リスクな出力まで「AI生成」と明示することが煩雑になり、かえって混乱を招く可能性。
- **形式的な対応に陥るリスク**：ディスクレーマー表示や研修が義務的・形式的になると、実質的なリテラシー向上につながらない。

### 挙げられた意見（掲示板より抜粋）
- 「AIによる判断が人間による判断と比較して優れていることが統計的に実証されている場合には、一時的判断をAIに委ねることの方が合理的ではないか」 – *jujunjun110*
- 「AIの出力についても人間または行政が最終的な責任を持つことは当然として、すべての出力を事前チェックする必要はないのではないか」 – *Takatsugu_Imaizumi*
- 「AIの判断と人間の判断がそれぞれどの程度間違いうるのかをきちんとデータを取って、エビデンスベースで政策決定すべき」 – *nishio*
- 「導入当初はアウトプットとコメントを照らし合わせてチェックするが、将来的には自走できる仕組みが必要」 – *tami*

### 海外事例
- **カリフォルニア州（AB 3030）**：医療におけるAI活用時、患者に「これはAIが生成した内容である」と明示することを義務づけ、人間による対応へのアクセスも保証。
- **サンフランシスコ市**：全ての市職員に対して、「AIによって生成された内容はファクトチェックを行い、使用時にAI利用を明示すること」を求めるガイドラインを策定。
- **ロサンゼルス郡・教育機関（LAUSD）**：生成AI利用時には生徒や職員に向けて、「AIで作成した内容はその旨を明記する」ことと「著作権への配慮」などの指針を設定。
- **EU（AI Act）**：高リスクAIに対しては透明性義務を課し、出力がAIによるものであることの明示、ならびに人間による説明責任の確保を求めている。


# その他の意見
- 「まず影響度が低く、導入しやすい分野（手続きやFAQなど）で早期に導入し、市民の理解と信頼を得るべき」 NakataniHibiki
- 「AI活用領域とプログラム実行領域を分けるDMZ的構造を導入し、安全な処理空間を守るべき」yamaokakitaro



# 補足情報
## 検討の進め方
- AIチャット、X、youtubeにて幅広く意見を収集
- Discourseを用いて作成した掲示板にて有識者が議論を実施
- githubにて提言を作成し、pullrequestを用いて参加者が提言の修正を提案することで提言をブラッシュアップ
- mkdocsで最新版の提言を表示
## 参考URL
- 有識者による議論を行った掲示板：https://large-scale-conversation-sandbox.discourse.group/t/topic/74
- AIチャット、掲示板、X、youtubeから収集した意見の取りまとめ：https://delib.takahiroanno.com/projects/67bdc8a41e9569d8678259b7
## 貢献してくれた人
- 追記予定

